{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T22:03:46.301240Z",
     "start_time": "2019-11-18T22:03:45.090154Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import interp\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed to ensure reproducability\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T22:03:46.312079Z",
     "start_time": "2019-11-18T22:03:46.304231Z"
    }
   },
   "outputs": [],
   "source": [
    "# Column names extracted from the dataset description file\n",
    "cols = [\"Elevation\", \"Aspect\", \"Slope\", \"Horizontal_Distance_To_Hydrology\", \n",
    "         \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Roadways\", \n",
    "         \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\", \n",
    "         \"Horizontal_Distance_To_Fire_Points\"] + \\\n",
    "       [\"Wilderness_Area_{}\".format(i) for i in range(4)] + \\\n",
    "       [\"Soil_Type {}\".format(i) for i in range(40)] + \\\n",
    "       [\"Cover_Type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T22:03:48.138433Z",
     "start_time": "2019-11-18T22:03:46.316880Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset/covtype.data', header=None, names=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "- **subsample the data to 10k samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T22:03:48.550420Z",
     "start_time": "2019-11-18T22:03:48.141031Z"
    }
   },
   "outputs": [],
   "source": [
    "class_size= 10000\n",
    "df = data.groupby(\"Cover_Type\").apply(lambda x: x.sample(min(class_size, len(x)))).reset_index(1)\n",
    "df = df.drop(columns=[\"Cover_Type\"]).reset_index().drop(columns=[\"level_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T22:09:01.233147Z",
     "start_time": "2019-11-18T22:09:01.210518Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df.loc[:, df.columns != 'Cover_Type']\n",
    "y = df[[\"Cover_Type\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the binary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T22:04:07.533318Z",
     "start_time": "2019-11-18T22:04:07.523285Z"
    }
   },
   "outputs": [],
   "source": [
    "binary_columns = df.columns[df.columns.map(lambda x: (\"Area\" in x) or (\"Soil\" in x))]\n",
    "binX = X.loc[:, binary_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T22:09:50.336617Z",
     "start_time": "2019-11-18T22:09:50.327354Z"
    }
   },
   "outputs": [],
   "source": [
    "numeric_columns = [\"Elevation\", \"Aspect\", \"Slope\", \"Horizontal_Distance_To_Hydrology\", \n",
    "         \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Roadways\", \n",
    "         \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\", \n",
    "         \"Horizontal_Distance_To_Fire_Points\"]\n",
    "numX = X.loc[:, numeric_columns]\n",
    "numX = pd.concat([numX, y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T22:11:52.684578Z",
     "start_time": "2019-11-18T22:11:52.598247Z"
    }
   },
   "outputs": [],
   "source": [
    "dumX = pd.DataFrame()\n",
    "\n",
    "for col in numX:\n",
    "    cutdf = pd.cut(numX[col], 3, labels=[0, 1, 2])\n",
    "    dummydf = pd.get_dummies(cutdf)\n",
    "    seriesname = dummydf.columns.name\n",
    "    dummydf.columns = [seriesname + \"_low\", seriesname + \"_mid\", seriesname + \"_high\"]\n",
    "    dumX = pd.concat([dumX, dummydf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T22:26:33.994836Z",
     "start_time": "2019-11-18T22:26:33.990255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dummyfied dataset: (62240, 33)\n",
      "Shape of the binary dataset:  (62240, 77)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the dummyfied dataset:\", dumX.shape)\n",
    "print(\"Shape of the binary dataset: \", allX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T22:11:53.296286Z",
     "start_time": "2019-11-18T22:11:53.276908Z"
    }
   },
   "outputs": [],
   "source": [
    "allX = pd.concat([dumX, binX], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T22:17:40.874068Z",
     "start_time": "2019-11-18T22:17:14.267796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 6 combinations | Sampling itemset size 2\n",
      "Minsup 0.81 found 3 patterns out of which 0 contain class consequent\n",
      "Processing 15 combinations | Sampling itemset size 3\n",
      "Minsup 0.7290000000000001 found 14 patterns out of which 0 contain class consequent\n",
      "Processing 4 combinations | Sampling itemset size 43\n",
      "Minsup 0.6561000000000001 found 23 patterns out of which 0 contain class consequent\n",
      "Processing 48 combinations | Sampling itemset size 43\n",
      "Minsup 0.5904900000000002 found 44 patterns out of which 0 contain class consequent\n",
      "Processing 60 combinations | Sampling itemset size 54\n",
      "Minsup 0.5314410000000002 found 67 patterns out of which 0 contain class consequent\n",
      "Processing 105 combinations | Sampling itemset size 5\n",
      "Minsup 0.47829690000000014 found 100 patterns out of which 0 contain class consequent\n",
      "Processing 36 combinations | Sampling itemset size 65\n",
      "Minsup 0.43046721000000016 found 146 patterns out of which 0 contain class consequent\n",
      "Processing 7 combinations | Sampling itemset size 7 6\n",
      "Minsup 0.38742048900000015 found 191 patterns out of which 0 contain class consequent\n",
      "Processing 21 combinations | Sampling itemset size 765\n",
      "Minsup 0.34867844010000015 found 268 patterns out of which 0 contain class consequent\n",
      "Processing 49 combinations | Sampling itemset size 765\n",
      "Minsup 0.31381059609000017 found 360 patterns out of which 0 contain class consequent\n",
      "Processing 8 combinations | Sampling itemset size 8 76\n",
      "Minsup 0.28242953648100017 found 479 patterns out of which 0 contain class consequent\n",
      "Processing 16 combinations | Sampling itemset size 876\n",
      "Minsup 0.25418658283290013 found 627 patterns out of which 0 contain class consequent\n",
      "Processing 24 combinations | Sampling itemset size 876\n",
      "Minsup 0.22876792454961012 found 809 patterns out of which 0 contain class consequent\n",
      "Processing 288 combinations | Sampling itemset size 87\n",
      "Minsup 0.2058911320946491 found 1060 patterns out of which 0 contain class consequent\n",
      "Minimum support: 0.2058911320946491\n",
      "Number of found patterns: 1060\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "frequent_itemsets = {}\n",
    "minpaterns = 1000\n",
    "minsup = 0.9\n",
    "classrules = None\n",
    "while minsup > 0:    \n",
    "    minsup = minsup * 0.9\n",
    "    frequent_itemsets = apriori(allX, min_support=minsup, use_colnames=True, verbose=3)\n",
    "    \n",
    "    if len(frequent_itemsets) < 1:\n",
    "        continue\n",
    "    \n",
    "    frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
    "    classrules = rules[rules['consequents'] == {'Cover_Type'}]\n",
    "    \n",
    "    print(\"Minsup {} found {} patterns out of which {} contain class consequent\"\n",
    "          .format(minsup, len(frequent_itemsets), len(classrules)))  \n",
    "    \n",
    "    if len(classrules) >= 100:\n",
    "        print(\"Found {} rules which have Cover_Type as consequent.\".format(len(classrules)))\n",
    "        break;\n",
    "    \n",
    "    if len(frequent_itemsets) >= minpaterns:\n",
    "        print(\"Minimum support:\", minsup)\n",
    "        print(\"Number of found patterns:\", len(frequent_itemsets))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Workflow of getting the rules into }\\LaTeX$$\n",
    "1. Generate this csv as shown below\n",
    "2. Copy it to the https://www.tablesgenerator.com/latex_tables and under File->Paste table data paste the data\n",
    "3. Copy the data to the Latex\n",
    "4. Replace \"-\" with \\\\ to do the line breaks\n",
    "5. Style the table and resize it correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T22:18:30.788484Z",
     "start_time": "2019-11-18T22:18:30.761592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antecedents;consequents;conf;supp;lever\n",
      "Aspect_low-Hillshade_Noon_high;Hillshade_9am_high;1.0;0.379;0.041\n",
      "Aspect_low-Slope_low;Hillshade_9am_high;1.0;0.368;0.04\n",
      "Aspect_low-Slope_low-Hillshade_Noon_high;Hillshade_9am_high;1.0;0.358;0.039\n",
      "Aspect_low-Hillshade_3pm_mid-Slope_low;Hillshade_9am_high;1.0;0.34;0.037\n",
      "Aspect_low-Hillshade_3pm_mid-Hillshade_Noon_high;Hillshade_9am_high;1.0;0.333;0.036\n",
      "\n"
     ]
    }
   ],
   "source": [
    "select = rules\n",
    "select = select.sort_values(by=[\"confidence\", \"support\", \"leverage\"], ascending=False)\\\n",
    "    [[\"antecedents\", \"consequents\", \"confidence\", \"support\", \"leverage\"]].head(5).round(3)\n",
    "\n",
    "select = select.rename(columns={\"confidence\": \"conf\", \"support\": \"supp\", \"leverage\": \"lever\"})\n",
    "print(select.to_csv(index=False, sep=\";\").replace(\"frozenset({\", \"\").replace(\"})\", \"\").replace(\"'\", \"\")\\\n",
    "     .replace(\", \", \"-\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "- include the silhouette analysis\n",
    "- whatever score\n",
    "- contingency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
